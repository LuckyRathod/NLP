{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word Embedding\n",
    "\n",
    "he Keras Embedding layer can also use a word embedding learned elsewhere.\n",
    "\n",
    "It is common in the field of Natural Language Processing to learn, save, and make freely available word embeddings.\n",
    "\n",
    "For example, the researchers behind GloVe method provide a suite of pre-trained word embeddings on their website released under a public domain license. See:\n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "The smallest package of embeddings is 822Mb, called “glove.6B.zip“. It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There are a few different embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n",
    "You can download this collection of embeddings and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your training dataset.\n",
    "\n",
    "After downloading and unzipping, you will see a few files, one of which is “glove.6B.100d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "If you peek inside the file, you will see a token (word) followed by the weights (100 numbers) on each line. For example, below are the first line of the embedding ASCII text file showing the embedding for “the“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tokenizer Keras similar to OneHot Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pad Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = max([len(sen.split(' ')) for sen in docs ])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove Word Embedding File as Dictionary of Word to embedding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "print(type(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53086  ,  0.51404  ,  0.087599 , -0.37314  ,  0.2747   ,\n",
       "        0.07947  , -0.0085023,  0.028399 , -0.35114  ,  0.094339 ,\n",
       "        0.087771 , -0.38307  ,  0.43129  ,  0.15261  , -0.1512   ,\n",
       "       -0.4607   ,  0.080433 ,  0.037627 , -0.43959  ,  0.42451  ,\n",
       "        0.16058  ,  0.26608  ,  0.35311  ,  0.014055 , -0.052771 ,\n",
       "       -0.1615   , -0.299    , -0.56214  , -0.18742  ,  0.044237 ,\n",
       "       -0.28118  ,  0.36594  , -0.26226  ,  0.11013  ,  0.44358  ,\n",
       "        0.43131  , -0.0053095,  0.34705  , -0.44883  , -0.33727  ,\n",
       "       -0.13281  , -0.35542  , -0.081663 , -0.12983  ,  0.080606 ,\n",
       "       -0.161    ,  0.367    , -0.30568  ,  0.057269 , -0.794    ,\n",
       "       -0.24581  ,  0.027115 ,  0.13203  ,  1.2262   , -0.19183  ,\n",
       "       -2.5497   ,  0.055273 , -0.1378   ,  1.4552   ,  0.53697  ,\n",
       "       -0.12337  ,  1.1278   , -0.16365  ,  0.21871  ,  0.82735  ,\n",
       "       -0.30681  ,  0.65456  ,  0.17636  ,  0.6172   , -0.18425  ,\n",
       "       -0.029966 , -0.098315 ,  0.32056  , -0.28124  ,  0.25684  ,\n",
       "       -0.034462 , -0.12968  ,  0.1944   , -0.42318  , -0.12843  ,\n",
       "        0.84729  ,  0.17807  , -0.39679  ,  0.29828  , -1.7337   ,\n",
       "       -0.037541 , -0.02989  , -0.14391  , -0.33299  , -0.52234  ,\n",
       "       -0.12178  , -0.2509   , -0.17904  ,  0.049504 , -0.62184  ,\n",
       "        0.20902  , -0.55805  , -0.55397  ,  0.56137  ,  0.39822  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['well']  ### there will be 100 d of word 'well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('work', 1), ('done', 2), ('good', 3), ('effort', 4), ('poor', 5), ('well', 6), ('great', 7), ('nice', 8), ('excellent', 9), ('weak', 10), ('not', 11), ('could', 12), ('have', 13), ('better', 14)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index.items() ### Each word has its own Integer value which is required before Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedded Matrix with GLOVE weigths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "print(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53086  ,  0.51404  ,  0.087599 , -0.37314  ,  0.2747   ,\n",
       "        0.07947  , -0.0085023,  0.028399 , -0.35114  ,  0.094339 ,\n",
       "        0.087771 , -0.38307  ,  0.43129  ,  0.15261  , -0.1512   ,\n",
       "       -0.4607   ,  0.080433 ,  0.037627 , -0.43959  ,  0.42451  ,\n",
       "        0.16058  ,  0.26608  ,  0.35311  ,  0.014055 , -0.052771 ,\n",
       "       -0.1615   , -0.299    , -0.56214  , -0.18742  ,  0.044237 ,\n",
       "       -0.28118  ,  0.36594  , -0.26226  ,  0.11013  ,  0.44358  ,\n",
       "        0.43131  , -0.0053095,  0.34705  , -0.44883  , -0.33727  ,\n",
       "       -0.13281  , -0.35542  , -0.081663 , -0.12983  ,  0.080606 ,\n",
       "       -0.161    ,  0.367    , -0.30568  ,  0.057269 , -0.794    ,\n",
       "       -0.24581  ,  0.027115 ,  0.13203  ,  1.2262   , -0.19183  ,\n",
       "       -2.5497   ,  0.055273 , -0.1378   ,  1.4552   ,  0.53697  ,\n",
       "       -0.12337  ,  1.1278   , -0.16365  ,  0.21871  ,  0.82735  ,\n",
       "       -0.30681  ,  0.65456  ,  0.17636  ,  0.6172   , -0.18425  ,\n",
       "       -0.029966 , -0.098315 ,  0.32056  , -0.28124  ,  0.25684  ,\n",
       "       -0.034462 , -0.12968  ,  0.1944   , -0.42318  , -0.12843  ,\n",
       "        0.84729  ,  0.17807  , -0.39679  ,  0.29828  , -1.7337   ,\n",
       "       -0.037541 , -0.02989  , -0.14391  , -0.33299  , -0.52234  ,\n",
       "       -0.12178  , -0.2509   , -0.17904  ,  0.049504 , -0.62184  ,\n",
       "        0.20902  , -0.55805  , -0.55397  ,  0.56137  ,  0.39822  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embeddings_index.get('well')\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now above Embedded vector of word 'well' will get replace in Main Embedded Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.11619   ,  0.45447001, -0.69216001, ..., -0.54737002,\n",
       "         0.48822001,  0.32246   ],\n",
       "       [-0.2978    ,  0.31147   , -0.14937   , ..., -0.22709   ,\n",
       "        -0.029261  ,  0.4585    ],\n",
       "       ...,\n",
       "       [ 0.05869   ,  0.40272999,  0.38633999, ..., -0.35973999,\n",
       "         0.43718001,  0.10121   ],\n",
       "       [ 0.15711001,  0.65605998,  0.0021149 , ..., -0.60614997,\n",
       "         0.71004999,  0.41468999],\n",
       "       [-0.047543  ,  0.51914001,  0.34283999, ..., -0.26859   ,\n",
       "         0.48664999,  0.55609   ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Above process will be done for each and every word . Its value will get stored in Embedded_Matrix\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer \n",
    "\n",
    "Now we will Directly provide the Embedded Matrix to the Embedding Layer which has weights from Glove\n",
    "\n",
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False.\n",
    "\n",
    "Here Learning will not be done , Becuase we have alreadY used pretrained glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lucky_Rathod\\Anaconda3\\envs\\kn_course\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6996 - acc: 0.5000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6882 - acc: 0.5000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.6772 - acc: 0.6000\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6664 - acc: 0.6000\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.6559 - acc: 0.6000\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.6458 - acc: 0.6000\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6359 - acc: 0.6000\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6263 - acc: 0.7000\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6170 - acc: 0.7000\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.6079 - acc: 0.7000\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5990 - acc: 0.7000\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5904 - acc: 0.7000\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5820 - acc: 0.9000\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 3ms/sample - loss: 0.5738 - acc: 0.9000\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.5658 - acc: 0.9000\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.5580 - acc: 0.9000\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5504 - acc: 0.9000\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5429 - acc: 0.9000\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5356 - acc: 0.9000\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.5284 - acc: 0.9000\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5214 - acc: 0.9000\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.5145 - acc: 0.9000\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5078 - acc: 0.9000\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.5012 - acc: 0.9000\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4947 - acc: 0.9000\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 3ms/sample - loss: 0.4883 - acc: 0.9000\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4821 - acc: 0.9000\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4759 - acc: 0.9000\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4699 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4639 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4581 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4524 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4468 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4413 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4358 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4305 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4252 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4201 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.4150 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 4ms/sample - loss: 0.4100 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4051 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.4002 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.3954 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.3908 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.3861 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.3816 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.3771 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.3727 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.3684 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 1ms/sample - loss: 0.3641 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x206a9d42ec8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(padded_docs, labels, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model1.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuract is 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
