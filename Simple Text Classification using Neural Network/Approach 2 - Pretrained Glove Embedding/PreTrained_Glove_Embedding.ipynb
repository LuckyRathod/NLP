{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word Embedding\n",
    "\n",
    "he Keras Embedding layer can also use a word embedding learned elsewhere.\n",
    "\n",
    "It is common in the field of Natural Language Processing to learn, save, and make freely available word embeddings.\n",
    "\n",
    "For example, the researchers behind GloVe method provide a suite of pre-trained word embeddings on their website released under a public domain license. See:\n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "The smallest package of embeddings is 822Mb, called “glove.6B.zip“. It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There are a few different embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n",
    "You can download this collection of embeddings and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your training dataset.\n",
    "\n",
    "After downloading and unzipping, you will see a few files, one of which is “glove.6B.100d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "If you peek inside the file, you will see a token (word) followed by the weights (100 numbers) on each line. For example, below are the first line of the embedding ASCII text file showing the embedding for “the“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tokenizer Keras similar to OneHot Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pad Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = max([len(sen.split(' ')) for sen in docs ])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove Word Embedding File as Dictionary of Word to embedding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "print(type(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53086  ,  0.51404  ,  0.087599 , -0.37314  ,  0.2747   ,\n",
       "        0.07947  , -0.0085023,  0.028399 , -0.35114  ,  0.094339 ,\n",
       "        0.087771 , -0.38307  ,  0.43129  ,  0.15261  , -0.1512   ,\n",
       "       -0.4607   ,  0.080433 ,  0.037627 , -0.43959  ,  0.42451  ,\n",
       "        0.16058  ,  0.26608  ,  0.35311  ,  0.014055 , -0.052771 ,\n",
       "       -0.1615   , -0.299    , -0.56214  , -0.18742  ,  0.044237 ,\n",
       "       -0.28118  ,  0.36594  , -0.26226  ,  0.11013  ,  0.44358  ,\n",
       "        0.43131  , -0.0053095,  0.34705  , -0.44883  , -0.33727  ,\n",
       "       -0.13281  , -0.35542  , -0.081663 , -0.12983  ,  0.080606 ,\n",
       "       -0.161    ,  0.367    , -0.30568  ,  0.057269 , -0.794    ,\n",
       "       -0.24581  ,  0.027115 ,  0.13203  ,  1.2262   , -0.19183  ,\n",
       "       -2.5497   ,  0.055273 , -0.1378   ,  1.4552   ,  0.53697  ,\n",
       "       -0.12337  ,  1.1278   , -0.16365  ,  0.21871  ,  0.82735  ,\n",
       "       -0.30681  ,  0.65456  ,  0.17636  ,  0.6172   , -0.18425  ,\n",
       "       -0.029966 , -0.098315 ,  0.32056  , -0.28124  ,  0.25684  ,\n",
       "       -0.034462 , -0.12968  ,  0.1944   , -0.42318  , -0.12843  ,\n",
       "        0.84729  ,  0.17807  , -0.39679  ,  0.29828  , -1.7337   ,\n",
       "       -0.037541 , -0.02989  , -0.14391  , -0.33299  , -0.52234  ,\n",
       "       -0.12178  , -0.2509   , -0.17904  ,  0.049504 , -0.62184  ,\n",
       "        0.20902  , -0.55805  , -0.55397  ,  0.56137  ,  0.39822  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['well']  ### there will be 100 d of word 'well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('work', 1), ('done', 2), ('good', 3), ('effort', 4), ('poor', 5), ('well', 6), ('great', 7), ('nice', 8), ('excellent', 9), ('weak', 10), ('not', 11), ('could', 12), ('have', 13), ('better', 14)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index.items() ### Each word has its own Integer value which is required before Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedded Matrix with GLOVE weigths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "print(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53086  ,  0.51404  ,  0.087599 , -0.37314  ,  0.2747   ,\n",
       "        0.07947  , -0.0085023,  0.028399 , -0.35114  ,  0.094339 ,\n",
       "        0.087771 , -0.38307  ,  0.43129  ,  0.15261  , -0.1512   ,\n",
       "       -0.4607   ,  0.080433 ,  0.037627 , -0.43959  ,  0.42451  ,\n",
       "        0.16058  ,  0.26608  ,  0.35311  ,  0.014055 , -0.052771 ,\n",
       "       -0.1615   , -0.299    , -0.56214  , -0.18742  ,  0.044237 ,\n",
       "       -0.28118  ,  0.36594  , -0.26226  ,  0.11013  ,  0.44358  ,\n",
       "        0.43131  , -0.0053095,  0.34705  , -0.44883  , -0.33727  ,\n",
       "       -0.13281  , -0.35542  , -0.081663 , -0.12983  ,  0.080606 ,\n",
       "       -0.161    ,  0.367    , -0.30568  ,  0.057269 , -0.794    ,\n",
       "       -0.24581  ,  0.027115 ,  0.13203  ,  1.2262   , -0.19183  ,\n",
       "       -2.5497   ,  0.055273 , -0.1378   ,  1.4552   ,  0.53697  ,\n",
       "       -0.12337  ,  1.1278   , -0.16365  ,  0.21871  ,  0.82735  ,\n",
       "       -0.30681  ,  0.65456  ,  0.17636  ,  0.6172   , -0.18425  ,\n",
       "       -0.029966 , -0.098315 ,  0.32056  , -0.28124  ,  0.25684  ,\n",
       "       -0.034462 , -0.12968  ,  0.1944   , -0.42318  , -0.12843  ,\n",
       "        0.84729  ,  0.17807  , -0.39679  ,  0.29828  , -1.7337   ,\n",
       "       -0.037541 , -0.02989  , -0.14391  , -0.33299  , -0.52234  ,\n",
       "       -0.12178  , -0.2509   , -0.17904  ,  0.049504 , -0.62184  ,\n",
       "        0.20902  , -0.55805  , -0.55397  ,  0.56137  ,  0.39822  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embeddings_index.get('well')\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now above Embedded vector of word 'well' will get replace in Main Embedded Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.11619   ,  0.45447001, -0.69216001, ..., -0.54737002,\n",
       "         0.48822001,  0.32246   ],\n",
       "       [-0.2978    ,  0.31147   , -0.14937   , ..., -0.22709   ,\n",
       "        -0.029261  ,  0.4585    ],\n",
       "       ...,\n",
       "       [ 0.05869   ,  0.40272999,  0.38633999, ..., -0.35973999,\n",
       "         0.43718001,  0.10121   ],\n",
       "       [ 0.15711001,  0.65605998,  0.0021149 , ..., -0.60614997,\n",
       "         0.71004999,  0.41468999],\n",
       "       [-0.047543  ,  0.51914001,  0.34283999, ..., -0.26859   ,\n",
       "         0.48664999,  0.55609   ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Above process will be done for each and every word . Its value will get stored in Embedded_Matrix\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer \n",
    "\n",
    "Now we will Directly provide the Embedded Matrix to the Embedding Layer which has weights from Glove\n",
    "\n",
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False.\n",
    "\n",
    "Here Learning will not be done , Becuase we have alreadY used pretrained glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lucky_Rathod\\Anaconda3\\envs\\kn_course\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lucky_Rathod\\Anaconda3\\envs\\kn_course\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Lucky_Rathod\\Anaconda3\\envs\\kn_course\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_docs, labels, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
