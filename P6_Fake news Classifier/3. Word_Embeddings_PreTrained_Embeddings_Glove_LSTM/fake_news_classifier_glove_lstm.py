# -*- coding: utf-8 -*-
"""fake_news_classifier_glove_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c0XcJBpnECcgO1GgF1JLPy85QNu55T-A

### Word Embeddings - Pretrained Embeddings - GLOVE -LSTM
"""

!unzip "glove.6B.100d.zip" -d "glove.6B.100d"

import pandas as pd

df = pd.read_csv('train.csv')
df.head()

df = df.dropna()
df.shape

## Get Independent Features
X = df.drop('label',axis=1)

## Get Dependent Feature
y = df['label']

print(X.shape,y.shape)

"""### Text Preprocessing or Data Preprocessing"""

messages = X.copy()
messages.reset_index(inplace=True)
messages.head()

import nltk
nltk.download('all')

import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer 
from nltk.stem import WordNetLemmatizer
import re
from nltk.corpus import stopwords
ps = PorterStemmer()
wn = WordNetLemmatizer()
corpus = []

for i in range(0,len(messages)):
    ## Substitute all character except a-zA-Z
    review = re.sub('[^a-zA-Z]',' ',messages['title'][i])
    review = review.lower()
    review = review.split()
    
    review = [wn.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review) ### It will make a complete sentence , If not written words will be splitted in each sentence
    corpus.append(review)
    
corpus

len(corpus)

"""### Pretrained Embeddings - Using GLOVE """

from numpy import array
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Embedding

"""### Instead of One Hot Representation we will use Tokenizer here"""

from tensorflow.keras.preprocessing.text import Tokenizer

t = Tokenizer()
t.fit_on_texts(corpus)
vocab_size = len(t.word_index) + 1
print('Vocab Size : ' , vocab_size)
# integer encode the documents
encoded_docs = t.texts_to_sequences(corpus)
print(encoded_docs)

"""### Using Pad Sequences"""

max_length = max([len(sen.split(' ')) for sen in corpus ])
max_length

# pad documents to a max length of 4 words
max_length = max([len(sen.split(' ')) for sen in corpus ])
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)

padded_docs.shape

"""### Load Glove Word Embedding File as Dictionary of Word to Embedding array of Features"""

# load the whole embedding into memory
from numpy import asarray
embeddings_index = dict()
f = open('glove.6B.100d/glove.6B.100d.txt',encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))
print(type(embeddings_index))

embeddings_index['house']  ### there will be 100 d of word 'house'

vocab_size

t.word_index.items() ### Each word has its own Integer value which is required before Embedding layer

"""###  Creating Embedding Matrix with Glove Weights

Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.
"""

from numpy import zeros
embedding_matrix = zeros((vocab_size, 100))
print(embedding_matrix)
embedding_matrix.shape

embedding_vector = embeddings_index.get('house')
embedding_vector

"""Now above embedded vector of 'house' will get replaced in Main Embedding matrix"""

### Above process will be done for each and every word . Its value will get stored in Embedded_Matrix

for word, i in t.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_matrix

"""### Embedding Layer

Now we will Directly provide the Embedded Matrix to the Embedding Layer which has weights from Glove

The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False.

Here Learning will not be done , Becuase we have alreadY used pretrained glove embedding
"""

max_length

embedding_matrix.shape

"""### In Embedding layer we will set trainable=False , Because we dont need to train over here as we are already providing trainable embedded matrix"""

e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

"""###  Creating Model"""

model = Sequential()
model.add(e)
model.add(LSTM(100))  ### 1 LSTM layer with 100 Neurons
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

padded_docs[0]

padded_docs.shape

import numpy as np

X_final = np.array(padded_docs)
y_final = np.array(y)

X_final.shape,y_final.shape

"""### Train Test Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)
X_train.shape,y_train.shape,X_test.shape,y_test.shape

"""### Model Training"""

### Finally Training
model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

"""### Performance metrics"""

y_pred=model.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### Simple RNN with Glove"""

from tensorflow.keras.layers import Dropout
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model0 = Sequential()
model0.add(e)
model0.add(Flatten())
model0.add(Dense(1, activation='sigmoid'))
# compile the model
model0.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# summarize the model
print(model0.summary())

model0.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model0.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### Adding Dropout layer with LSTM"""

from tensorflow.keras.layers import Dropout
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model1 = Sequential()
model1.add(e)
model1.add(Dropout(0.3))
model1.add(LSTM(100))  ### 1 LSTM layer with 100 Neurons
model1.add(Dense(1,activation='sigmoid'))
model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model1.summary())

model1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model1.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### Using Bidirectional LSTM"""

from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Bidirectional
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model2 = Sequential()
model2.add(e)
model2.add(Bidirectional(LSTM(100)))  ### 1 LSTM layer with 100 Neurons
model2.add(Dense(1,activation='sigmoid'))
model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model2.summary())

model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model2.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### Bidirectional LSTM and Dropout layer"""

from tensorflow.keras.layers import Dropout
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model3 = Sequential()
model3.add(e)
model3.add(Dropout(0.3))
model3.add(Bidirectional(LSTM(100)))  ### 1 LSTM layer with 100 Neurons
model3.add(Dense(1,activation='sigmoid'))
model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model3.summary())

model3.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model3.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### 2 LSTM - Dropout - Glove """

from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Bidirectional
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model4 = Sequential()
model4.add(e)
model4.add(Dropout(0.3))
model4.add(LSTM(100,return_sequences=True))  ### 1 LSTM layer with 100 Neurons
model4.add(Dropout(0.3))
model4.add(LSTM(100))
model4.add(Dense(1,activation='sigmoid'))
model4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model4.summary())

model4.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model4.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""### Stacked Bidirectional LSTM - Dropout - Glove"""

from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Bidirectional
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)

model5 = Sequential()
model5.add(e)
model5.add(Dropout(0.3))
model5.add(Bidirectional(LSTM(128, recurrent_dropout=0.4, return_sequences=True)))
model5.add(Dropout(0.3))
model5.add(Bidirectional(LSTM(128, recurrent_dropout=0.4)))
model5.add(Dense(1,activation='sigmoid'))
model5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model5.summary())

model5.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred=model5.predict_classes(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

